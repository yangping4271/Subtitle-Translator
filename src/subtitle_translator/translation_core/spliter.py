import difflib
import re
from concurrent.futures import ThreadPoolExecutor
from typing import List
from .split_by_llm import split_by_llm
from .data import SubtitleData, SubtitleSegment, save_split_results
from .config import get_default_config
from ..logger import setup_logger

logger = setup_logger("subtitle_merger")

FIXED_NUM_THREADS = 1  # å›ºå®šçš„çº¿ç¨‹æ•°é‡
SPLIT_RANGE = 30  # åœ¨åˆ†å‰²ç‚¹å‰åå¯»æ‰¾æœ€å¤§æ—¶é—´é—´éš”çš„èŒƒå›´
MAX_GAP = 1500  # å…è®¸æ¯ä¸ªè¯è¯­ä¹‹é—´çš„æœ€å¤§æ—¶é—´é—´éš” ms

class SubtitleProcessError(Exception):
    """å­—å¹•å¤„ç†ç›¸å…³çš„å¼‚å¸¸"""
    pass

class SmartSplitError(Exception):
    """æ™ºèƒ½æ–­å¥å¼‚å¸¸"""
    def __init__(self, message: str, suggestion: str = ""):
        self.message = message
        self.suggestion = suggestion
        super().__init__(message)
    
    def __str__(self):
        return self.message

class TranslationError(Exception):
    """ç¿»è¯‘å¼‚å¸¸"""
    def __init__(self, message: str, suggestion: str = ""):
        self.message = message
        self.suggestion = suggestion
        super().__init__(message)
    
    def __str__(self):
        return self.message

class SummaryError(Exception):
    """å†…å®¹åˆ†æå¼‚å¸¸"""
    def __init__(self, message: str, suggestion: str = ""):
        self.message = message
        self.suggestion = suggestion
        super().__init__(message)
    
    def __str__(self):
        return self.message

class EmptySubtitleError(Exception):
    """ç©ºå­—å¹•æ–‡ä»¶å¼‚å¸¸ - è¿™æ˜¯ä¸€ä¸ªé¢„æœŸçš„æƒ…å†µï¼Œä¸éœ€è¦è¯¦ç»†é”™è¯¯å †æ ˆ"""
    def __init__(self, message: str):
        self.message = message
        super().__init__(message)
    
    def __str__(self):
        return self.message

def is_pure_punctuation(s: str) -> bool:
    """
    æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä»…ç”±æ ‡ç‚¹ç¬¦å·ç»„æˆ
    """
    return not re.search(r'\w', s, flags=re.UNICODE)


def count_words(text: str) -> int:
    """
    ç»Ÿè®¡å¤šè¯­è¨€æ–‡æœ¬ä¸­çš„å­—ç¬¦/å•è¯æ•°
    """
    # å®šä¹‰å„ç§è¯­è¨€çš„UnicodeèŒƒå›´
    patterns = [
        r'[\u4e00-\u9fff]',           # ä¸­æ—¥éŸ©ç»Ÿä¸€è¡¨æ„æ–‡å­—
        r'[\u3040-\u309f]',           # å¹³å‡å
        r'[\u30a0-\u30ff]',           # ç‰‡å‡å
        r'[\uac00-\ud7af]',           # éŸ©æ–‡éŸ³èŠ‚
        r'[\u0e00-\u0e7f]',           # æ³°æ–‡
        r'[\u0600-\u06ff]',           # é˜¿æ‹‰ä¼¯æ–‡
        r'[\u0400-\u04ff]',           # è¥¿é‡Œå°”å­—æ¯ï¼ˆä¿„æ–‡ç­‰ï¼‰
        r'[\u0590-\u05ff]',           # å¸Œä¼¯æ¥æ–‡
        r'[\u1e00-\u1eff]',           # è¶Šå—æ–‡
        r'[\u3130-\u318f]',           # éŸ©æ–‡å…¼å®¹å­—æ¯
    ]
    
    # ç»Ÿè®¡æ‰€æœ‰éè‹±æ–‡å­—ç¬¦
    non_english_chars = 0
    remaining_text = text
    
    for pattern in patterns:
        # è®¡ç®—å½“å‰è¯­è¨€çš„å­—ç¬¦æ•°
        chars = len(re.findall(pattern, remaining_text))
        non_english_chars += chars
        # ä»æ–‡æœ¬ä¸­ç§»é™¤å·²è®¡æ•°çš„å­—ç¬¦
        remaining_text = re.sub(pattern, ' ', remaining_text)
    
    # è®¡ç®—è‹±æ–‡å•è¯æ•°ï¼ˆå¤„ç†å‰©ä½™çš„æ–‡æœ¬ï¼‰
    english_words = len(remaining_text.strip().split())
    
    return non_english_chars + english_words


def preprocess_text(s: str) -> str:
    """
    é€šè¿‡è§„èŒƒåŒ–ç©ºæ ¼æ¥æ ‡å‡†åŒ–æ–‡æœ¬
    """
    return ' '.join(s.split())


def merge_segments_based_on_sentences(segments: List[SubtitleSegment], sentences: List[str], max_unmatched: int = 5) -> List[SubtitleSegment]:
    """
    åŸºäºæä¾›çš„å¥å­åˆ—è¡¨åˆå¹¶å­—å¹•åˆ†æ®µ
    
    Args:
        segments: å­—å¹•æ®µåˆ—è¡¨
        sentences: å¥å­åˆ—è¡¨
        max_unmatched: å…è®¸çš„æœ€å¤§æœªåŒ¹é…å¥å­æ•°é‡ï¼Œè¶…è¿‡æ­¤æ•°é‡å°†æŠ›å‡ºå¼‚å¸¸
        
    Returns:
        åˆå¹¶åçš„ SubtitleSegment åˆ—è¡¨
        
    Raises:
        SubtitleProcessError: å½“æœªåŒ¹é…å¥å­æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶æŠ›å‡º
    """
    asr_texts = [seg.text for seg in segments]
    asr_len = len(asr_texts)
    asr_index = 0  # å½“å‰åˆ†æ®µç´¢å¼•ä½ç½®
    threshold = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
    max_shift = 30  # æ»‘åŠ¨çª—å£çš„æœ€å¤§åç§»é‡
    unmatched_count = 0  # æœªåŒ¹é…å¥å­è®¡æ•°

    new_segments = []

    for sentence in sentences:
        sentence_proc = preprocess_text(sentence)
        word_count = count_words(sentence_proc)
        best_ratio = 0.0
        best_pos = None
        best_window_size = 0

        # æ»‘åŠ¨çª—å£å¤§å°ï¼Œä¼˜å…ˆè€ƒè™‘æ¥è¿‘å¥å­è¯æ•°çš„çª—å£
        max_window_size = min(word_count * 2, asr_len - asr_index)
        min_window_size = max(1, word_count // 2)
        window_sizes = sorted(range(min_window_size, max_window_size + 1), key=lambda x: abs(x - word_count))

        for window_size in window_sizes:
            max_start = min(asr_index + max_shift + 1, asr_len - window_size + 1)
            for start in range(asr_index, max_start):
                substr = ''.join(asr_texts[start:start + window_size])
                substr_proc = preprocess_text(substr)
                ratio = difflib.SequenceMatcher(None, sentence_proc, substr_proc).ratio()

                if ratio > best_ratio:
                    best_ratio = ratio
                    best_pos = start
                    best_window_size = window_size
                if ratio == 1.0:
                    break  # å®Œå…¨åŒ¹é…
            if best_ratio == 1.0:
                break  # å®Œå…¨åŒ¹é…

        if best_ratio >= threshold and best_pos is not None:
            start_seg_index = best_pos
            end_seg_index = best_pos + best_window_size - 1
            
            segs_to_merge = segments[start_seg_index:end_seg_index + 1]

            # æŒ‰ç…§æ—¶é—´åˆ‡åˆ†é¿å…åˆå¹¶è·¨åº¦å¤§çš„
            seg_groups = merge_by_time_gaps(segs_to_merge, max_gap=MAX_GAP)

            for group in seg_groups:
                # ç›´æ¥ä½¿ç”¨LLMè¿”å›çš„åŸå§‹å¥å­ï¼Œå®Œå…¨ä¿ç•™æ ¼å¼å’Œæ ‡ç‚¹
                merged_text = sentence_proc
                
                merged_start_time = group[0].start_time
                merged_end_time = group[-1].end_time
                merged_seg = SubtitleSegment(merged_text, merged_start_time, merged_end_time)
                
                new_segments.append(merged_seg)
            
            max_shift = 30
            asr_index = end_seg_index + 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœªå¤„ç†çš„åˆ†æ®µ
        else:
            logger.warning(f"æ— æ³•åŒ¹é…å¥å­: {sentence}")
            unmatched_count += 1
            if unmatched_count > max_unmatched:
                logger.error(f"æœªåŒ¹é…å¥å­æ•°é‡è¶…è¿‡é˜ˆå€¼ ({max_unmatched})ï¼Œè¿”å›åŸå§‹åˆ†æ®µ")
                return segments
            max_shift = 100
            asr_index = min(asr_index + 1, asr_len - 1)  # ç¡®ä¿ä¸ä¼šè¶…å‡ºèŒƒå›´
    
    # å¦‚æœæ²¡æœ‰æˆåŠŸåŒ¹é…ä»»ä½•å¥å­ï¼Œè¿”å›åŸå§‹åˆ†æ®µ
    if not new_segments:
        logger.warning("æ²¡æœ‰æˆåŠŸåŒ¹é…ä»»ä½•å¥å­ï¼Œè¿”å›åŸå§‹åˆ†æ®µ")
        return segments

    return new_segments

def merge_short_segment(segments: List[SubtitleSegment]) -> None:
    """
    åˆå¹¶è¿‡çŸ­çš„åˆ†æ®µ
    """
    if not segments:  # æ·»åŠ ç©ºåˆ—è¡¨æ£€æŸ¥
        return
        
    i = 0  # ä»å¤´å¼€å§‹éå†
    while i < len(segments) - 1:  # ä¿®æ”¹éå†æ–¹å¼
        current_seg = segments[i]
        next_seg = segments[i + 1]
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå¹¶:
        # 1. æ—¶é—´é—´éš”å°äº300ms
        # 2. å½“å‰æ®µè½æˆ–ä¸‹ä¸€æ®µè½è¯æ•°å°äº5
        # 3. åˆå¹¶åæ€»è¯æ•°ä¸è¶…è¿‡é™åˆ¶
        time_gap = abs(next_seg.start_time - current_seg.end_time)
        current_words = count_words(current_seg.text)
        next_words = count_words(next_seg.text)
        total_words = current_words + next_words
        config = get_default_config()
        max_word_count = config.max_word_count_english

        if time_gap < 300 and (current_words < 5 or next_words <= 5) \
            and total_words <= max_word_count \
            and ("." not in current_seg.text and "?" not in current_seg.text and "!" not in current_seg.text):
            # æ‰§è¡Œåˆå¹¶æ“ä½œ
            logger.info(f"åˆå¹¶ä¼˜åŒ–: {current_seg.text} --- {next_seg.text}") 
            # æ›´æ–°å½“å‰æ®µè½çš„æ–‡æœ¬å’Œç»“æŸæ—¶é—´
            current_seg.text += " " + next_seg.text
            current_seg.end_time = next_seg.end_time
            
            # ä»åˆ—è¡¨ä¸­ç§»é™¤ä¸‹ä¸€ä¸ªæ®µè½
            segments.pop(i + 1)
            # ä¸å¢åŠ iï¼Œå› ä¸ºéœ€è¦ç»§ç»­æ£€æŸ¥åˆå¹¶åçš„æ®µè½
        else:
            i += 1


def preprocess_segments(segments: List[SubtitleSegment]) -> List[SubtitleSegment]:
    """
    é¢„å¤„ç†å­—å¹•åˆ†æ®µ:
    1. ç§»é™¤çº¯æ ‡ç‚¹ç¬¦å·çš„åˆ†æ®µ
    2. ä¿ç•™åŸå§‹å¤§å°å†™æ ¼å¼
    
    Args:
        segments: å­—å¹•åˆ†æ®µåˆ—è¡¨
    Returns:
        å¤„ç†åçš„åˆ†æ®µåˆ—è¡¨
    """
    new_segments = []
    for seg in segments:
        if not is_pure_punctuation(seg.text):
            # ä¿ç•™åŸå§‹æ ¼å¼ï¼Œä¸è½¬æ¢ä¸ºå°å†™
            new_segments.append(seg)
    return new_segments


def merge_by_time_gaps(segments: List[SubtitleSegment], max_gap: int = MAX_GAP, check_large_gaps: bool = False) -> List[List[SubtitleSegment]]:
    """
    æ ¹æ®æ—¶é—´é—´éš”åˆå¹¶åˆ†æ®µ
    """
    if not segments:
        return []
    
    result = []
    current_group = [segments[0]]
    recent_gaps = []  # å­˜å‚¨æœ€è¿‘çš„æ—¶é—´é—´éš”
    WINDOW_SIZE = 5   # æ£€æŸ¥æœ€è¿‘5ä¸ªé—´éš”
    
    for i in range(1, len(segments)):
        time_gap = segments[i].start_time - segments[i-1].end_time
        
        if check_large_gaps:
            recent_gaps.append(time_gap)
            if len(recent_gaps) > WINDOW_SIZE:
                recent_gaps.pop(0)
            if len(recent_gaps) == WINDOW_SIZE:
                avg_gap = sum(recent_gaps) / len(recent_gaps)
                # å¦‚æœå½“å‰é—´éš”å¤§äºå¹³å‡å€¼çš„3å€
                if time_gap > avg_gap*3 and len(current_group) > 5:
                    result.append(current_group)
                    current_group = []
                    recent_gaps = []  # é‡ç½®é—´éš”è®°å½•
        
        if time_gap > max_gap:
            result.append(current_group)
            current_group = []
            recent_gaps = []  # é‡ç½®é—´éš”è®°å½•
            
        current_group.append(segments[i])
    
    if current_group:
        result.append(current_group)
    
    return result


def process_by_llm(segments: List[SubtitleSegment], 
                   model: str = None,
                   max_word_count_english: int = None,
                   batch_index: int = None) -> List[SubtitleSegment]:
    """
    ä½¿ç”¨LLMå¤„ç†å­—å¹•åˆ†æ®µï¼Œè¿›è¡Œæ‹†åˆ†å’Œåˆå¹¶
    
    Args:
        segments: å­—å¹•åˆ†æ®µåˆ—è¡¨
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
        max_word_count_english: è‹±æ–‡æœ€å¤§å•è¯æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
        batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        
    Returns:
        List[SubtitleSegment]: å¤„ç†åçš„å­—å¹•åˆ†æ®µåˆ—è¡¨
    """
    config = get_default_config()
    max_word_count_english = max_word_count_english or config.max_word_count_english
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
    if model is None:
        model = config.split_model
        
    # ä¿®æ”¹åˆå¹¶æ–‡æœ¬çš„æ–¹å¼ï¼Œæ·»åŠ ç©ºæ ¼
    txt = " ".join([seg.text.strip() for seg in segments])
    # è®°å½•å½“å‰æ‰¹æ¬¡çš„å•è¯æ•°
    current_words = count_words(txt)
    batch_prefix = f"[æ‰¹æ¬¡{batch_index}]" if batch_index else ""
    logger.debug(f"ğŸ“ {batch_prefix} å¤„ç† {current_words} ä¸ªå•è¯")
    
    # ä½¿ç”¨LLMæ‹†åˆ†å¥å­
    sentences = split_by_llm(txt, 
                           model=model, 
                           max_word_count_english=max_word_count_english,
                           batch_index=batch_index)
    logger.debug(f"âœ‚ï¸ {batch_prefix} æå– {len(sentences)} ä¸ªå¥å­")
    
    # å¯¹å½“å‰åˆ†æ®µè¿›è¡Œåˆå¹¶å¤„ç†
    merged_segments = merge_segments_based_on_sentences(segments, sentences)
    return merged_segments


def split_by_sentences(asr_data: SubtitleData, word_threshold: int = 500) -> List[SubtitleData]:
    """
    æ ¹æ®å¥å·ç­‰æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†å¥å­ï¼Œå¹¶æŒ‰æŒ‡å®šå•è¯æ•°é˜ˆå€¼åˆ†ç»„
    
    Args:
        asr_data: å­—å¹•æ•°æ®
        word_threshold: æ¯ç»„æœ€å¤§å•è¯æ•°ï¼Œé»˜è®¤500
        
    Returns:
        List[SubtitleData]: æŒ‰å•è¯æ•°é˜ˆå€¼åˆ†ç»„åçš„å­—å¹•æ•°æ®åˆ—è¡¨
    """
    # å®šä¹‰å¥å­ç»“æŸæ ‡å¿—
    sentence_end_markers = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦']
    # å®šä¹‰åˆ†å¥æ ‡ç‚¹
    split_markers = [',', 'ï¼Œ', ';', 'ï¼›', 'ã€']
    
    # é¢„å¤„ç†å­—å¹•æ•°æ®
    segments = preprocess_segments(asr_data.segments)
    
    # æŒ‰å¥å­åˆ‡åˆ†
    sentence_segments = []
    current_sentence_segments = []
    
    for seg in segments:
        current_sentence_segments.append(seg)
        text = seg.text.strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­ç»“å°¾
        if any(text.endswith(marker) for marker in sentence_end_markers):
            if current_sentence_segments:
                sentence_segments.append(current_sentence_segments)
                current_sentence_segments = []
    
    # å¤„ç†æœ€åä¸€ç»„æœªå®Œæˆçš„å¥å­
    if current_sentence_segments:
        sentence_segments.append(current_sentence_segments)
    
    # æŒ‰å•è¯æ•°é˜ˆå€¼åˆ†ç»„
    batched_data = []
    current_batch = []
    current_segments = []
    current_word_count = 0
    
    def split_long_sentence(sentence_segs: List[SubtitleSegment]) -> List[List[SubtitleSegment]]:
        """æ‹†åˆ†è¿‡é•¿çš„å¥å­"""
        result = []
        temp_segs = []
        temp_word_count = 0
        
        for seg in sentence_segs:
            seg_text = seg.text.strip()
            seg_word_count = count_words(seg_text)
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šä¹‹å‰çš„å·²ç»è¶…è¿‡é˜ˆå€¼ï¼Œå¹¶ä¸”å½“å‰æ®µè½ä»¥åˆ†å¥æ ‡ç‚¹ç»“å°¾
            if (temp_word_count + seg_word_count > word_threshold and 
                any(seg_text.endswith(marker) for marker in split_markers)):
                if temp_segs:
                    result.append(temp_segs)
                    temp_segs = []
                    temp_word_count = 0
            
            temp_segs.append(seg)
            temp_word_count += seg_word_count
            
            # å¦‚æœç´¯ç§¯çš„å•è¯æ•°å·²ç»æ¥è¿‘é˜ˆå€¼ï¼Œå¼ºåˆ¶åˆ†æ®µ
            if temp_word_count >= word_threshold * 1.2:
                if temp_segs:
                    result.append(temp_segs)
                    temp_segs = []
                    temp_word_count = 0
        
        # å¤„ç†å‰©ä½™çš„æ®µè½
        if temp_segs:
            result.append(temp_segs)
        
        return result
    
    for sentence in sentence_segments:
        # è®¡ç®—å½“å‰å¥å­çš„å•è¯æ•°
        sentence_text = " ".join([seg.text for seg in sentence])
        sentence_word_count = count_words(sentence_text)
        
        # å¦‚æœå½“å‰å¥å­è¶…è¿‡é˜ˆå€¼ï¼Œå°è¯•æ‹†åˆ†
        if sentence_word_count >= word_threshold:
            # å…ˆä¿å­˜å½“å‰æ‰¹æ¬¡
            if current_segments:
                batched_data.append(SubtitleData(current_segments))
                current_batch = []
                current_segments = []
                current_word_count = 0
            
            # æ‹†åˆ†é•¿å¥å­
            split_parts = split_long_sentence(sentence)
            for part in split_parts:
                batched_data.append(SubtitleData(part))
            continue
            
        # å¦‚æœæ·»åŠ å½“å‰å¥å­åè¶…è¿‡é˜ˆå€¼ï¼Œå…ˆä¿å­˜å½“å‰æ‰¹æ¬¡ï¼Œç„¶åå¼€å§‹æ–°æ‰¹æ¬¡
        if current_word_count + sentence_word_count > word_threshold and current_segments:
            batched_data.append(SubtitleData(current_segments))
            current_batch = []
            current_segments = []
            current_word_count = 0
        
        current_batch.append(sentence)
        current_segments.extend(sentence)
        current_word_count += sentence_word_count
    
    # å¤„ç†æœ€åä¸€æ‰¹æœªæ»¡çš„æ•°æ®
    if current_segments:
        batched_data.append(SubtitleData(current_segments))
    
    return batched_data


def merge_segments(asr_data: SubtitleData, 
                   model: str = None, 
                   num_threads: int = FIXED_NUM_THREADS, 
                   save_split: str = None) -> SubtitleData:
    """
    åˆå¹¶å­—å¹•åˆ†æ®µ
    
    Args:
        asr_data: å­—å¹•æ•°æ®
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
        num_threads: çº¿ç¨‹æ•°é‡
        save_split: ä¿å­˜æ–­å¥ç»“æœçš„æ–‡ä»¶è·¯å¾„
    """
    import time
    from concurrent.futures import ThreadPoolExecutor
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
    if model is None:
        config = get_default_config()
        model = config.split_model
    
    # é¢„å¤„ç†å­—å¹•æ•°æ®ï¼Œç§»é™¤çº¯æ ‡ç‚¹ç¬¦å·çš„åˆ†æ®µï¼Œå¹¶å¤„ç†ä»…åŒ…å«å­—æ¯å’Œæ’‡å·çš„æ–‡æœ¬
    asr_data.segments = preprocess_segments(asr_data.segments)
    
    # ä½¿ç”¨æ–°çš„æŒ‰å•è¯æ•°åˆ†ç»„æ–¹æ³•
    word_threshold = 500
    asr_data_segments = split_by_sentences(asr_data, word_threshold=word_threshold)
    total_segments = len(asr_data_segments)
    
    # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
    logger.info(f"ğŸ“‹ æ‰¹æ¬¡è§„åˆ’: æ¯ç»„{word_threshold}å­—ï¼Œå…± {total_segments} ä¸ªæ‰¹æ¬¡")
    
    # æ˜¾ç¤ºæ‰¹æ¬¡åˆ†å¸ƒï¼ˆç®€åŒ–ï¼‰
    batch_info = []
    for i, segment in enumerate(asr_data_segments):
        segment_text = " ".join([seg.text.strip() for seg in segment.segments])
        word_count = count_words(segment_text)
        batch_info.append(f"æ‰¹æ¬¡{i+1}: {word_count}å­—")
    
    logger.debug(f"æ‰¹æ¬¡è¯¦æƒ…: {', '.join(batch_info)}")
    logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œæ–­å¥å¤„ç†...")
    
    # å¤šçº¿ç¨‹å¤„ç†æ¯ä¸ªåˆ†æ®µ
    all_segments = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        def process_segment(args):
            index, asr_data_part = args
            try:
                return process_by_llm(asr_data_part.segments, model=model, batch_index=index+1)
            except SmartSplitError as e:
                # æ™ºèƒ½æ–­å¥å¼‚å¸¸ï¼Œç›´æ¥æŠ›å‡ºä¸é‡å¤åŒ…è£…
                raise e
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {index+1} å¤„ç†å¤±è´¥: {str(e)}")
                raise Exception(f"æ‰¹æ¬¡ {index+1} å¤„ç†å¤±è´¥: {str(e)}")

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ†æ®µï¼Œæ·»åŠ æ‰¹æ¬¡ç¼–å·
        try:
            processed_segments = list(executor.map(process_segment, enumerate(asr_data_segments)))
        except Exception as e:
            logger.error(f"ğŸ’¥ å¹¶è¡Œå¤„ç†å¤±è´¥: {str(e)}")
            raise

    # åˆå¹¶æ‰€æœ‰å¤„ç†åçš„åˆ†æ®µ
    for i, segment in enumerate(processed_segments):
        all_segments.extend(segment)
        logger.debug(f"ğŸ“ˆ å¤„ç†è¿›åº¦: {((i+1)/len(processed_segments)*100):.0f}% ({i+1}/{len(processed_segments)})")

    all_segments.sort(key=lambda seg: seg.start_time)

    # å¦‚æœéœ€è¦ä¿å­˜æ–­å¥ç»“æœ
    if save_split:
        try:
            # è·å–è¾“å…¥çš„å…¨éƒ¨æ–‡æœ¬
            all_text = asr_data.to_txt()
            # è·å–æ‰€æœ‰å¤„ç†åçš„åˆ†æ®µæ–‡æœ¬
            split_sentences = [seg.text for seg in all_segments]
            
            # æ˜¾ç¤ºæ–­å¥ç»“æœ
            save_split_results(all_text, split_sentences, save_split)
            logger.info(f"ğŸ“„ æ–­å¥ç»“æœå·²ä¿å­˜åˆ°: {save_split}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–­å¥ç»“æœå¤±è´¥: {str(e)}")

    merge_short_segment(all_segments)

    # åˆ›å»ºæœ€ç»ˆçš„å­—å¹•æ•°æ®å¯¹è±¡
    final_asr_data = SubtitleData(all_segments)

    processing_time = time.time() - start_time
    logger.info(f"âœ… æ‰€æœ‰æ–­å¥å®Œæˆ! å…± {len(all_segments)} å¥ï¼Œè€—æ—¶ {processing_time:.1f}ç§’")

    return final_asr_data