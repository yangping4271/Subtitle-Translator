import difflib
import re
from typing import List, Optional

from ..logger import setup_logger
from .batch_utils import calculate_batch_sizes
from .config import get_default_config
from .data import PreSplitSentence, SubtitleSegment
from .split_by_llm import split_by_llm

logger = setup_logger("subtitle_merger")

# å¸¸é‡å®šä¹‰
MAX_GAP = 1500  # å…è®¸æ¯ä¸ªè¯è¯­ä¹‹é—´çš„æœ€å¤§æ—¶é—´é—´éš” ms
MIN_SENTENCE_WORDS = 3  # åˆ†å¥çš„æœ€å°å•è¯æ•°
MIN_LAST_SEGMENT_WORDS = 2  # æœ€åä¸€æ®µçš„æœ€å°å•è¯æ•°
SIMILARITY_THRESHOLD = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
MAX_SHIFT = 30  # æ»‘åŠ¨çª—å£çš„æœ€å¤§åç§»é‡
MAX_UNMATCHED_SENTENCES = 5  # å…è®¸çš„æœ€å¤§æœªåŒ¹é…å¥å­æ•°é‡
SHORT_SEGMENT_TIME_GAP = 300  # çŸ­åˆ†æ®µåˆå¹¶çš„æ—¶é—´é—´éš”é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
SHORT_SEGMENT_MIN_WORDS = 5  # çŸ­åˆ†æ®µçš„æœ€å°å•è¯æ•°

def is_pure_punctuation(s: str) -> bool:
    """
    æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä»…ç”±æ ‡ç‚¹ç¬¦å·ç»„æˆ
    """
    return not re.search(r'\w', s, flags=re.UNICODE)


def count_words(text: str) -> int:
    """
    ç»Ÿè®¡å¤šè¯­è¨€æ–‡æœ¬ä¸­çš„å­—ç¬¦/å•è¯æ•°
    """
    # å®šä¹‰å„ç§è¯­è¨€çš„UnicodeèŒƒå›´
    patterns = [
        r'[\u4e00-\u9fff]',           # ä¸­æ—¥éŸ©ç»Ÿä¸€è¡¨æ„æ–‡å­—
        r'[\u3040-\u309f]',           # å¹³å‡å
        r'[\u30a0-\u30ff]',           # ç‰‡å‡å
        r'[\uac00-\ud7af]',           # éŸ©æ–‡éŸ³èŠ‚
        r'[\u0e00-\u0e7f]',           # æ³°æ–‡
        r'[\u0600-\u06ff]',           # é˜¿æ‹‰ä¼¯æ–‡
        r'[\u0400-\u04ff]',           # è¥¿é‡Œå°”å­—æ¯ï¼ˆä¿„æ–‡ç­‰ï¼‰
        r'[\u0590-\u05ff]',           # å¸Œä¼¯æ¥æ–‡
        r'[\u1e00-\u1eff]',           # è¶Šå—æ–‡
        r'[\u3130-\u318f]',           # éŸ©æ–‡å…¼å®¹å­—æ¯
    ]
    
    # ç»Ÿè®¡æ‰€æœ‰éè‹±æ–‡å­—ç¬¦
    non_english_chars = 0
    remaining_text = text
    
    for pattern in patterns:
        # è®¡ç®—å½“å‰è¯­è¨€çš„å­—ç¬¦æ•°
        chars = len(re.findall(pattern, remaining_text))
        non_english_chars += chars
        # ä»æ–‡æœ¬ä¸­ç§»é™¤å·²è®¡æ•°çš„å­—ç¬¦
        remaining_text = re.sub(pattern, ' ', remaining_text)
    
    # è®¡ç®—è‹±æ–‡å•è¯æ•°ï¼ˆå¤„ç†å‰©ä½™çš„æ–‡æœ¬ï¼‰
    english_words = len(remaining_text.strip().split())
    
    return non_english_chars + english_words


def preprocess_text(s: str) -> str:
    """
    é€šè¿‡è§„èŒƒåŒ–ç©ºæ ¼æ¥æ ‡å‡†åŒ–æ–‡æœ¬
    """
    return ' '.join(s.split())


def split_by_end_marks(sentence: str) -> List[str]:
    """
    æŒ‰å¥å­ç»“æŸæ ‡è®°æ‹†åˆ†å¥å­ï¼ˆç§»æ¤è‡ª youtube-subtitleï¼‰

    è§„åˆ™ï¼š
    - æŒ‰ '. ' '! ' '? ' '.' '!' '?' åˆ†å¥
    - è·³è¿‡å°æ•°ç‚¹ï¼ˆæ£€æµ‹å‰ä¸€ä¸ªå­—ç¬¦æ˜¯å¦ä¸ºæ•°å­—ï¼‰
    - æ¯æ®µè‡³å°‘ MIN_SENTENCE_WORDS ä¸ªå•è¯æ‰åˆ†å‰²
    - æœ€åä¸€æ®µå°‘äº MIN_LAST_SEGMENT_WORDS ä¸ªå•è¯æ—¶ï¼Œåˆå¹¶åˆ°å‰ä¸€æ®µ

    Args:
        sentence: è¾“å…¥æ–‡æœ¬

    Returns:
        æ‹†åˆ†åçš„å¥å­åˆ—è¡¨
    """
    end_marks = ['. ', '! ', '? ', '.', '!', '?']
    positions = []

    for mark in end_marks:
        start = 0
        while True:
            pos = sentence.find(mark, start)
            if pos == -1:
                break

            if _is_decimal_point(sentence, pos, mark):
                start = pos + 1
                continue

            positions.append(pos + len(mark))
            start = pos + 1

    if not positions:
        return [sentence]

    unique_positions = sorted(set(positions))
    segments = []
    start = 0

    for pos in unique_positions:
        segment = sentence[start:pos].strip()
        if segment and count_words(segment) >= MIN_SENTENCE_WORDS:
            segments.append(segment)
            start = pos

    last_segment = sentence[start:].strip()
    if last_segment:
        if segments and count_words(last_segment) < MIN_LAST_SEGMENT_WORDS:
            segments[-1] = segments[-1] + ' ' + last_segment
        else:
            segments.append(last_segment)

    return segments if len(segments) > 1 else [sentence]


def _is_decimal_point(sentence: str, pos: int, mark: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºå°æ•°ç‚¹"""
    return (mark == '. ' or mark == '.') and pos > 0 and sentence[pos - 1].isdigit()


def presplit_by_punctuation(word_segments: List[SubtitleSegment]) -> List[PreSplitSentence]:
    """
    åŸºäºæ ‡ç‚¹é¢„åˆ†å¥ï¼ˆç§»æ¤è‡ª youtube-subtitleï¼‰

    Args:
        word_segments: å•è¯çº§å­—å¹•æ®µåˆ—è¡¨

    Returns:
        PreSplitSentence åˆ—è¡¨ï¼ŒåŒ…å«å¥å­æ–‡æœ¬å’Œå¯¹åº”çš„å•è¯ç´¢å¼•èŒƒå›´
    """
    if not word_segments:
        return []

    # æ‹¼æ¥æ‰€æœ‰å•è¯ä¸ºå®Œæ•´æ–‡æœ¬
    full_text = ' '.join(seg.text for seg in word_segments)

    # ä½¿ç”¨ split_by_end_marks è¿›è¡Œé¢„åˆ†å¥
    sentences = split_by_end_marks(full_text)

    pre_split_sentences = []
    current_word_index = 0

    for sentence in sentences:
        sentence_words = sentence.strip().split()
        word_count = len(sentence_words)

        # è®¡ç®—å•è¯ç´¢å¼•èŒƒå›´
        word_start_index = current_word_index
        word_end_index = current_word_index + word_count

        # è·å–æ—¶é—´èŒƒå›´
        start_time = word_segments[word_start_index].start_time if word_start_index < len(word_segments) else 0
        end_time = word_segments[min(word_end_index - 1, len(word_segments) - 1)].end_time if word_end_index > 0 else 0

        pre_split_sentences.append(PreSplitSentence(
            text=sentence,
            word_start_index=word_start_index,
            word_end_index=word_end_index,
            start_time=start_time,
            end_time=end_time
        ))

        current_word_index = word_end_index

    return pre_split_sentences


def batch_by_sentence_count(
    sentences: List[PreSplitSentence],
    min_size: int = 15,
    max_size: int = 25
) -> List[List[PreSplitSentence]]:
    """
    æŒ‰å¥å­æ•°åˆ†æ‰¹ï¼ˆç§»æ¤è‡ª youtube-subtitleï¼Œç§»é™¤é¦–æ‰¹ç‰¹æ®Šå¤„ç†ï¼‰

    Args:
        sentences: é¢„åˆ†å¥åˆ—è¡¨
        min_size: æœ€å°æ‰¹æ¬¡å¤§å°
        max_size: æœ€å¤§æ‰¹æ¬¡å¤§å°

    Returns:
        æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡æ˜¯ PreSplitSentence åˆ—è¡¨
    """
    if not sentences:
        return []

    # è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼ˆä¸ä½¿ç”¨é¦–æ‰¹ç‰¹æ®Šå¤„ç†ï¼‰
    target_size = (min_size + max_size) // 2
    batch_sizes = calculate_batch_sizes(len(sentences), target_size, min_size, max_size)

    # æŒ‰è®¡ç®—å‡ºçš„æ‰¹æ¬¡å¤§å°åˆ†æ‰¹
    batches = []
    start_index = 0
    for size in batch_sizes:
        batches.append(sentences[start_index:start_index + size])
        start_index += size

    return batches


def merge_segments_within_batch(
    pre_split_sentences: List[PreSplitSentence],
    word_segments: List[SubtitleSegment],
    model: Optional[str] = None,
    batch_index: Optional[int] = None
) -> List[SubtitleSegment]:
    """
    åœ¨æ‰¹æ¬¡å†…è¿›è¡Œ LLM æ–­å¥å’Œæ—¶é—´æˆ³å¯¹é½ï¼ˆç§»æ¤è‡ª youtube-subtitleï¼‰

    Args:
        pre_split_sentences: æ‰¹æ¬¡å†…çš„é¢„åˆ†å¥åˆ—è¡¨
        word_segments: å®Œæ•´çš„å•è¯çº§å­—å¹•æ®µï¼ˆç”¨äºæ—¶é—´æˆ³å¯¹é½ï¼‰
        model: LLM æ¨¡å‹åç§°
        batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        å¤„ç†åçš„å­—å¹•æ®µåˆ—è¡¨
    """
    if not pre_split_sentences:
        return []

    config = get_default_config()
    if model is None:
        model = config.split_model

    # æå–æ‰¹æ¬¡å¯¹åº”çš„å•è¯ç‰‡æ®µ
    start_index = pre_split_sentences[0].word_start_index
    end_index = pre_split_sentences[-1].word_end_index
    batch_word_segments = word_segments[start_index:end_index]

    # æ‹¼æ¥ä¸ºæ–‡æœ¬
    batch_text = ' '.join(seg.text for seg in batch_word_segments)

    # è®°å½•æ—¥å¿—
    current_words = count_words(batch_text)
    batch_prefix = f"[æ‰¹æ¬¡{batch_index}]" if batch_index is not None else ""
    logger.info(f"ğŸ“ {batch_prefix} å¤„ç† {current_words} ä¸ªå•è¯ï¼Œ{len(pre_split_sentences)} ä¸ªé¢„åˆ†å¥")

    # LLM æ–­å¥
    llm_sentences = split_by_llm(
        batch_text,
        model=model,
        max_word_count_english=config.max_word_count_english,
        batch_index=batch_index
    )
    logger.info(f"âœ‚ï¸ {batch_prefix} LLM æ–­å¥å¾—åˆ° {len(llm_sentences)} ä¸ªå¥å­")

    # æ—¶é—´æˆ³å¯¹é½
    aligned_segments = merge_segments_based_on_sentences(batch_word_segments, llm_sentences)

    # åˆå¹¶è¿‡çŸ­çš„åˆ†æ®µ
    merge_short_segment(aligned_segments)

    return aligned_segments



def merge_segments_based_on_sentences(segments: List[SubtitleSegment], sentences: List[str], max_unmatched: int = MAX_UNMATCHED_SENTENCES) -> List[SubtitleSegment]:
    """
    åŸºäºæä¾›çš„å¥å­åˆ—è¡¨åˆå¹¶å­—å¹•åˆ†æ®µ
    
    Args:
        segments: å­—å¹•æ®µåˆ—è¡¨
        sentences: å¥å­åˆ—è¡¨
        max_unmatched: å…è®¸çš„æœ€å¤§æœªåŒ¹é…å¥å­æ•°é‡ï¼Œè¶…è¿‡æ­¤æ•°é‡å°†æŠ›å‡ºå¼‚å¸¸
        
    Returns:
        åˆå¹¶åçš„ SubtitleSegment åˆ—è¡¨
        
    Raises:
        SubtitleProcessError: å½“æœªåŒ¹é…å¥å­æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶æŠ›å‡º
    """
    asr_texts = [seg.text for seg in segments]
    asr_len = len(asr_texts)
    asr_index = 0
    unmatched_count = 0
    new_segments = []
    max_shift = MAX_SHIFT

    for sentence in sentences:
        sentence_proc = preprocess_text(sentence)
        word_count = count_words(sentence_proc)
        best_ratio = 0.0
        best_pos = None
        best_window_size = 0

        max_window_size = min(word_count * 2, asr_len - asr_index)
        min_window_size = max(1, word_count // 2)
        window_sizes = sorted(range(min_window_size, max_window_size + 1), key=lambda x: abs(x - word_count))

        for window_size in window_sizes:
            max_start = min(asr_index + max_shift + 1, asr_len - window_size + 1)
            for start in range(asr_index, max_start):
                substr = ''.join(asr_texts[start:start + window_size])
                substr_proc = preprocess_text(substr)
                ratio = difflib.SequenceMatcher(None, sentence_proc, substr_proc).ratio()

                if ratio > best_ratio:
                    best_ratio = ratio
                    best_pos = start
                    best_window_size = window_size
                if ratio == 1.0:
                    break
            if best_ratio == 1.0:
                break

        if best_ratio >= SIMILARITY_THRESHOLD and best_pos is not None:
            start_seg_index = best_pos
            end_seg_index = best_pos + best_window_size - 1

            segs_to_merge = segments[start_seg_index:end_seg_index + 1]
            seg_groups = merge_by_time_gaps(segs_to_merge, max_gap=MAX_GAP)

            for group in seg_groups:
                merged_text = sentence_proc
                merged_start_time = group[0].start_time
                merged_end_time = group[-1].end_time
                merged_seg = SubtitleSegment(merged_text, merged_start_time, merged_end_time)
                new_segments.append(merged_seg)

            max_shift = MAX_SHIFT
            asr_index = end_seg_index + 1
        else:
            logger.warning(f"æ— æ³•åŒ¹é…å¥å­: {sentence}")
            unmatched_count += 1
            if unmatched_count > max_unmatched:
                logger.error(f"æœªåŒ¹é…å¥å­æ•°é‡è¶…è¿‡é˜ˆå€¼ ({max_unmatched})ï¼Œè¿”å›åŸå§‹åˆ†æ®µ")
                return segments
            max_shift = 100
            asr_index = min(asr_index + 1, asr_len - 1)

    if not new_segments:
        logger.warning("æ²¡æœ‰æˆåŠŸåŒ¹é…ä»»ä½•å¥å­ï¼Œè¿”å›åŸå§‹åˆ†æ®µ")
        return segments

    return new_segments

def merge_short_segment(segments: List[SubtitleSegment]) -> None:
    """
    åˆå¹¶è¿‡çŸ­çš„åˆ†æ®µ
    """
    if not segments:  # æ·»åŠ ç©ºåˆ—è¡¨æ£€æŸ¥
        return
        
    i = 0  # ä»å¤´å¼€å§‹éå†
    while i < len(segments) - 1:  # ä¿®æ”¹éå†æ–¹å¼
        current_seg = segments[i]
        next_seg = segments[i + 1]
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå¹¶:
        # 1. æ—¶é—´é—´éš”å°äº300ms
        # 2. å½“å‰æ®µè½æˆ–ä¸‹ä¸€æ®µè½è¯æ•°å°äº5
        # 3. åˆå¹¶åæ€»è¯æ•°ä¸è¶…è¿‡é™åˆ¶
        time_gap = abs(next_seg.start_time - current_seg.end_time)
        current_words = count_words(current_seg.text)
        next_words = count_words(next_seg.text)
        total_words = current_words + next_words
        config = get_default_config()
        max_word_count = config.max_word_count_english

        if time_gap < 300 and (current_words < 5 or next_words <= 5) \
            and total_words <= max_word_count \
            and ("." not in current_seg.text and "?" not in current_seg.text and "!" not in current_seg.text):
            # æ‰§è¡Œåˆå¹¶æ“ä½œ
            logger.info(f"åˆå¹¶ä¼˜åŒ–: {current_seg.text} --- {next_seg.text}") 
            # æ›´æ–°å½“å‰æ®µè½çš„æ–‡æœ¬å’Œç»“æŸæ—¶é—´
            current_seg.text += " " + next_seg.text
            current_seg.end_time = next_seg.end_time
            
            # ä»åˆ—è¡¨ä¸­ç§»é™¤ä¸‹ä¸€ä¸ªæ®µè½
            segments.pop(i + 1)
            # ä¸å¢åŠ iï¼Œå› ä¸ºéœ€è¦ç»§ç»­æ£€æŸ¥åˆå¹¶åçš„æ®µè½
        else:
            i += 1


def preprocess_segments(segments: List[SubtitleSegment]) -> List[SubtitleSegment]:
    """
    é¢„å¤„ç†å­—å¹•åˆ†æ®µ:
    1. ç§»é™¤çº¯æ ‡ç‚¹ç¬¦å·çš„åˆ†æ®µ
    2. ä¿ç•™åŸå§‹å¤§å°å†™æ ¼å¼
    
    Args:
        segments: å­—å¹•åˆ†æ®µåˆ—è¡¨
    Returns:
        å¤„ç†åçš„åˆ†æ®µåˆ—è¡¨
    """
    new_segments = []
    for seg in segments:
        if not is_pure_punctuation(seg.text):
            # ä¿ç•™åŸå§‹æ ¼å¼ï¼Œä¸è½¬æ¢ä¸ºå°å†™
            new_segments.append(seg)
    return new_segments


def merge_by_time_gaps(segments: List[SubtitleSegment], max_gap: int = MAX_GAP, check_large_gaps: bool = False) -> List[List[SubtitleSegment]]:
    """
    æ ¹æ®æ—¶é—´é—´éš”åˆå¹¶åˆ†æ®µ
    """
    if not segments:
        return []

    result = []
    current_group = [segments[0]]
    recent_gaps = []  # å­˜å‚¨æœ€è¿‘çš„æ—¶é—´é—´éš”
    WINDOW_SIZE = 5   # æ£€æŸ¥æœ€è¿‘5ä¸ªé—´éš”

    for i in range(1, len(segments)):
        time_gap = segments[i].start_time - segments[i-1].end_time

        if check_large_gaps:
            recent_gaps.append(time_gap)
            if len(recent_gaps) > WINDOW_SIZE:
                recent_gaps.pop(0)
            if len(recent_gaps) == WINDOW_SIZE:
                avg_gap = sum(recent_gaps) / len(recent_gaps)
                # å¦‚æœå½“å‰é—´éš”å¤§äºå¹³å‡å€¼çš„3å€
                if time_gap > avg_gap*3 and len(current_group) > 5:
                    result.append(current_group)
                    current_group = []
                    recent_gaps = []  # é‡ç½®é—´éš”è®°å½•

        if time_gap > max_gap:
            result.append(current_group)
            current_group = []
            recent_gaps = []  # é‡ç½®é—´éš”è®°å½•

        current_group.append(segments[i])

    if current_group:
        result.append(current_group)

    return result