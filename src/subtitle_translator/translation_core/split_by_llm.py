import re
from typing import List
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI

from .data import SubtitleSegment
from .prompts import SPLIT_SYSTEM_PROMPT
from .config import SubtitleConfig, get_default_config
from ..logger import setup_logger

logger = setup_logger("split_by_llm")

def _extract_error_message(error_str: str) -> str:
    """æå–é”™è¯¯ä¿¡æ¯ä¸­çš„æ ¸å¿ƒå†…å®¹"""
    # æå– API é”™è¯¯ä¿¡æ¯
    if "Error code:" in error_str and "message" in error_str:
        try:
            # å°è¯•æå– JSON ä¸­çš„ message å­—æ®µ
            import json
            import re
            
            # æŸ¥æ‰¾ JSON éƒ¨åˆ†
            json_match = re.search(r'\{.*\}', error_str)
            if json_match:
                try:
                    error_data = json.loads(json_match.group())
                    if "error" in error_data and "message" in error_data["error"]:
                        return error_data["error"]["message"]
                except:
                    pass
        except:
            pass
    
    # å¦‚æœæ— æ³•è§£æ JSONï¼Œè¿”å›ç®€åŒ–çš„é”™è¯¯ä¿¡æ¯
    if "is not a valid model ID" in error_str:
        return "æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨"
    elif "401" in error_str or "Unauthorized" in error_str:
        return "APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ"
    elif "403" in error_str or "Forbidden" in error_str:
        return "APIè®¿é—®è¢«æ‹’ç»"
    elif "429" in error_str or "rate limit" in error_str.lower():
        return "APIè°ƒç”¨é¢‘ç‡é™åˆ¶"
    elif "timeout" in error_str.lower():
        return "è¯·æ±‚è¶…æ—¶"
    elif "connection" in error_str.lower():
        return "ç½‘ç»œè¿æ¥å¤±è´¥"
    else:
        # è¿”å›å‰50ä¸ªå­—ç¬¦ä½œä¸ºç®€åŒ–é”™è¯¯ä¿¡æ¯
        return error_str[:50] + ("..." if len(error_str) > 50 else "")

def _get_error_suggestions(error_str: str, model: str) -> str:
    """æ ¹æ®é”™è¯¯ç±»å‹è¿”å›é’ˆå¯¹æ€§å»ºè®®"""
    if "is not a valid model ID" in error_str:
        return f"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ¨¡å‹åç§° '{model}' æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ›´æ¢å…¶ä»–å¯ç”¨æ¨¡å‹"
    elif "401" in error_str or "Unauthorized" in error_str:
        return "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®"
    elif "403" in error_str:
        return "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ API å¯†é’¥æƒé™æˆ–è´¦æˆ·çŠ¶æ€"
    elif "429" in error_str or "rate limit" in error_str.lower():
        return "ğŸ’¡ å»ºè®®ï¼šç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ API è°ƒç”¨é¢‘ç‡é™åˆ¶"
    elif "timeout" in error_str.lower():
        return "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–å°è¯•ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹"
    elif "connection" in error_str.lower():
        return "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ API ç«¯ç‚¹è®¾ç½®"
    else:
        return "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ã€API å¯†é’¥å’Œæ¨¡å‹é…ç½®"

def count_words(text: str) -> int:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­è‹±æ–‡å•è¯æ•°
    Args:
        text: è¾“å…¥æ–‡æœ¬ï¼Œè‹±æ–‡
    Returns:
        int: è‹±æ–‡å•è¯æ•°
    """
    english_text = re.sub(r'[\u4e00-\u9fff]', ' ', text)
    english_words = english_text.strip().split()
    return len(english_words)

def split_by_end_marks(sentence: str) -> List[str]:
    """
    æŒ‰å¥å­ç»“æŸæ ‡è®°ï¼ˆå¥å·ã€æ„Ÿå¹å·ç­‰ï¼‰æ‹†åˆ†å¥å­ï¼Œåªè¦æœ‰ç»“æŸæ ‡è®°å°±åˆ†å‰²
    
    Args:
        sentence: éœ€è¦æ‹†åˆ†çš„å¥å­
        
    Returns:
        List[str]: æ‹†åˆ†åçš„å¥å­åˆ—è¡¨
    """
    # å®šä¹‰ç»“æŸæ ‡è®°ï¼Œæ³¨æ„æ¯ä¸ªæ ‡è®°åé¢éƒ½åŠ äº†ç©ºæ ¼
    end_marks = [". ", "! ", "? ", "... ", "â€¦â€¦ ", "; ", "? ", "! "]
    positions = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æŸæ ‡è®°çš„ä½ç½®
    for mark in end_marks:
        start = 0
        while True:
            pos = sentence.find(mark, start)
            if pos == -1:
                break
            # ç¡®ä¿ä¸æ˜¯å°æ•°ç‚¹
            if mark == ". " and pos > 0:
                prev_char = sentence[pos-1]
                if prev_char.isdigit():
                    start = pos + 1
                    continue
            positions.append((pos + len(mark.strip()), mark))
            start = pos + 1
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œè¿”å›åŸå¥å­
    if not positions:
        return [sentence]
    
    # æŒ‰ä½ç½®æ’åºå¹¶æ‰§è¡Œåˆ†å‰²
    positions.sort()
    segments = []
    start = 0
    
    for pos, mark in positions:
        segment = sentence[start:pos].strip()
        if segment:  # åªè¦ä¸æ˜¯ç©ºå­—ç¬¦ä¸²å°±æ·»åŠ 
            segments.append(segment)
        start = pos
    
    # æ·»åŠ æœ€åä¸€æ®µ
    last_segment = sentence[start:].strip()
    if last_segment:
        segments.append(last_segment)
    
    # è®°å½•æ—¥å¿—
    if len(segments) > 1:
        logger.info(f"æ‹†åˆ†ä¼˜åŒ–: {' -- '.join(segments)}")
    
    return segments if segments else [sentence]

def split_by_llm(text: str,
                model: str = None,
                max_word_count_english: int = 14,
                max_retries: int = 3,
                batch_index: int = None) -> List[str]:
    """
    ä½¿ç”¨LLMæ‹†åˆ†å¥å­
    
    Args:
        text: è¦æ‹†åˆ†çš„æ–‡æœ¬
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
        max_word_count_english: è‹±æ–‡æœ€å¤§å•è¯æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        
    Returns:
        List[str]: æ‹†åˆ†åçš„å¥å­åˆ—è¡¨
    """
    logger.info(f"ğŸ“ å¤„ç†æ–‡æœ¬: å…±{count_words(text)}ä¸ªå•è¯")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    config = SubtitleConfig()
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
    if model is None:
        model = config.split_model
    
    client = OpenAI(
        base_url=config.openai_base_url,
        api_key=config.openai_api_key
    )
    
    # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯
    system_prompt = SPLIT_SYSTEM_PROMPT.replace("[max_word_count_english]", str(max_word_count_english))
    
    # åœ¨ç”¨æˆ·æç¤ºä¸­æ·»åŠ å¯¹ç©ºæ ¼çš„å¼ºè°ƒ
    user_prompt = f"Please use multiple <br> tags to separate the following sentence. Make sure to preserve all spaces and punctuation exactly as they appear in the original text:\n{text}"

    try:
        # è°ƒç”¨API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.2,
            timeout=80
        )
        
        # å¤„ç†å“åº”
        result = response.choices[0].message.content
        if not result:
            raise Exception("APIè¿”å›ä¸ºç©º")
        logger.debug(f"APIè¿”å›ç»“æœ: \n\n{result}\n")

        # æ¸…ç†å’Œåˆ†å‰²æ–‡æœ¬ - ç®€åŒ–å¤„ç†ï¼Œä¿ç•™åŸå§‹æ ¼å¼
        result = re.sub(r'\n+', '', result)
        
        # ç›´æ¥æŒ‰<br>åˆ†å‰²ï¼Œä¿ç•™åŸå§‹æ ¼å¼å’Œç©ºæ ¼
        sentences = result.split("<br>")
        
        # æ¸…ç†ç©ºç™½è¡Œï¼Œä½†ä¿ç•™å†…éƒ¨ç©ºæ ¼
        sentences = [seg.strip() for seg in sentences if seg.strip()]

        # éªŒè¯å¥å­é•¿åº¦
        new_sentences = []
        long_sentence_count = 0
        super_long_count = 0
        
        for sentence in sentences:
            # é¦–å…ˆæŒ‰ç»“æŸæ ‡è®°æ‹†åˆ†å¥å­
            segments = split_by_end_marks(sentence)
            
            # å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œé•¿åº¦æ£€æŸ¥
            for segment in segments:
                threshold = max_word_count_english + 5
                word_count = count_words(segment)
                
                if max_word_count_english < word_count < threshold:
                    long_sentence_count += 1
                    logger.debug(f"âš ï¸ é•¿å¥: {word_count}å­— - {segment[:30]}...")
                    new_sentences.append(segment)
                elif word_count > threshold:
                    logger.info(f"ğŸ”„ å¤„ç†è¶…é•¿å¥: {word_count}å­— - {segment[:30]}...")
                    
                    # è®°å½•åˆ†å‰²å‰çš„åŸå§‹å†…å®¹ç”¨äºæ£€æŸ¥
                    original_segment = segment
                    
                    # å°è¯•åˆ‡åˆ†å¥å­
                    split_results = split_by_common_words(segment)
                    
                    # æ£€æŸ¥æ˜¯å¦å®é™…åˆ†å‰²æˆåŠŸ
                    if len(split_results) > 1:
                        super_long_count += 1
                        logger.info(f"âœ… è¶…é•¿å¥åˆ†å‰²æˆåŠŸ: {len(split_results)} ä¸ªç‰‡æ®µ")
                    else:
                        logger.warning(f"âš ï¸ è¶…é•¿å¥åˆ†å‰²å¤±è´¥ï¼Œä¿æŒåŸæ ·")
                    
                    new_sentences.extend(split_results)
                else:
                    new_sentences.append(segment)
        
        sentences = new_sentences

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if long_sentence_count > 0:
            logger.info(f"ğŸ“Š å‘ç° {long_sentence_count} ä¸ªé•¿å¥ (15-19å­—)")
        if super_long_count > 0:
            logger.info(f"âœ‚ï¸ æˆåŠŸåˆ†å‰² {super_long_count} ä¸ªè¶…é•¿å¥ (>19å­—)")

        # éªŒè¯ç»“æœ
        word_count = count_words(text)
        expected_segments = word_count / max_word_count_english
        actual_segments = len(sentences)
        
        if actual_segments < expected_segments * 0.9:
            logger.warning(f"âš ï¸ æ–­å¥æ•°é‡ä¸è¶³ï¼šé¢„æœŸ {expected_segments:.1f}ï¼Œå®é™… {actual_segments}")
        
        batch_prefix = f"[æ‰¹æ¬¡{batch_index}]" if batch_index else ""
        logger.info(f"âœ… {batch_prefix} æ–­å¥å®Œæˆ: {len(sentences)} ä¸ªå¥å­")
        return sentences
        
    except Exception as e:
        if max_retries > 0:
            logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œç¬¬{4-max_retries}æ¬¡é‡è¯•: {_extract_error_message(str(e))}")
            return split_by_llm(text, model, max_word_count_english, max_retries-1, batch_index)
        else:
            error_msg = _extract_error_message(str(e))
            logger.error(f"âŒ æ™ºèƒ½æ–­å¥å¤±è´¥: {error_msg}")
            
            # æ ¹æ®é”™è¯¯ç±»å‹ç»™å‡ºé’ˆå¯¹æ€§å»ºè®®
            suggestions = _get_error_suggestions(str(e), model)
            
            # åˆ›å»ºä¸€ä¸ªæºå¸¦å»ºè®®çš„è‡ªå®šä¹‰å¼‚å¸¸ç±»å‹
            from .spliter import SmartSplitError
            raise SmartSplitError(error_msg, suggestions)
        
def split_by_common_words(text: str) -> List[str]:
    """
    åœ¨å¸¸è§è¿æ¥è¯å¤„å¯¹å¥å­è¿›è¡Œåˆ†å‰²

    Args:
        text: éœ€è¦åˆ†å‰²çš„å¥å­
    Returns:
        åˆ†å‰²åçš„å¥å­åˆ—è¡¨ï¼Œå¦‚æœæ™ºèƒ½åˆ†å‰²å¤±è´¥åˆ™ä½¿ç”¨å¼ºåˆ¶åˆ†å‰²
    """
    # å®šä¹‰åœ¨è¯è¯­å‰é¢åˆ†å‰²çš„å¸¸è§è¯
    prefix_split_words = {
        # è¿æ¥è¯å’Œä»‹è¯
        "and", "or", "but", "if", "then", "because", "as", "until",
        "while", "what", "when", "where", "nor", "yet", "so", "for",
        "however", "moreover", "furthermore", "additionally", "besides",
        "therefore", "thus", "hence", "consequently",
        # ä»å¥å¼•å¯¼è¯
        "that", "which", "who", "whom", "whose", "why", "how",
        # æ—¶é—´å’Œæ¡ä»¶
        "before", "after", "since", "while", "unless", "although",
        "though", "even though", "whereas", "whether",
        # ç›®çš„å’Œç»“æœ
        "in order to", "so that", "such that", "in case", "provided that",
        # å¯¹æ¯”å’Œè®©æ­¥
        "despite", "in spite of", "rather than", "instead of",
        # è¡¥å……å’Œé€’è¿›
        "also", "besides", "in addition", "moreover", "furthermore",
        "similarly", "likewise", "meanwhile", "subsequently"
    }

    # å®šä¹‰åœ¨è¯è¯­åé¢åˆ†å‰²çš„å¸¸è§è¯
    suffix_split_words = {
        # æ ‡ç‚¹ç¬¦å·
        ".", ",", "!", "?", ":", ";", "...", "â€¦",
        # å¼•å·å’Œæ‹¬å·
        "\"", "'", "'", "'", "'", "'", ")", "]", "}"
    }

    # é¢„å¤„ç†æ–‡æœ¬
    text = text.strip()
    words = text.split()
    
    # å¦‚æœå¥å­å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
    if len(words) < 8:
        return [text]
        
    # å¯»æ‰¾åˆ†å‰²ç‚¹
    split_positions = []
    for i, word in enumerate(words):
        word_lower = word.lower().strip(",.!?")
        # æ£€æŸ¥å‰ç¼€åˆ†å‰²è¯
        if i > 2 and i < len(words) - 2:  # ç¡®ä¿ä¸åœ¨å¥å­å¼€å¤´å’Œç»“å°¾å¤ªè¿‘çš„ä½ç½®
            # æ£€æŸ¥å•è¯å’ŒçŸ­è¯­
            for prefix in prefix_split_words:
                if " " in prefix:  # å¤„ç†å¤šè¯çŸ­è¯­
                    if i + len(prefix.split()) <= len(words):
                        phrase = " ".join(words[i:i+len(prefix.split())]).lower()
                        if phrase == prefix:
                            split_positions.append(i)
                            break
                elif word_lower == prefix:  # å¤„ç†å•ä¸ªè¯
                    split_positions.append(i)
                    break
        
        # æ£€æŸ¥åç¼€åˆ†å‰²è¯
        if i > 2 and i < len(words) - 1:  # ç¡®ä¿ä¸åœ¨å¥å­å¼€å¤´å¤ªè¿‘çš„ä½ç½®
            if word_lower in suffix_split_words:
                split_positions.append(i + 1)  # åœ¨åç¼€è¯ä¹‹ååˆ†å‰²
    
    # å°è¯•æ™ºèƒ½åˆ†å‰²
    result = []
    if split_positions:
        # æ’åºå¹¶å»é‡åˆ†å‰²ç‚¹
        split_positions = sorted(list(set(split_positions)))
        
        # æ‰§è¡Œåˆ†å‰²
        start = 0
        for pos in split_positions:
            if pos - start >= 3:  # ç¡®ä¿æ¯ä¸ªåˆ†æ®µè‡³å°‘æœ‰3ä¸ªè¯
                segment = " ".join(words[start:pos])
                if segment:
                    result.append(segment)
                start = pos
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†æ®µ
        if start < len(words):
            last_segment = " ".join(words[start:])
            if last_segment:
                result.append(last_segment)
        
        # æ£€æŸ¥æ™ºèƒ½åˆ†å‰²ç»“æœ
        if len(result) > 1:
            # å¦‚æœæœ‰å¤šäºä¸¤ä¸ªåˆ†æ®µï¼Œå°è¯•åˆå¹¶æœ€çŸ­çš„ç›¸é‚»åˆ†æ®µ
            while len(result) > 2:
                # æ‰¾å‡ºæœ€çŸ­çš„ç›¸é‚»åˆ†æ®µå¯¹
                min_length = float('inf')
                merge_index = 0
                
                for i in range(len(result) - 1):
                    current_len = count_words(result[i]) + count_words(result[i + 1])
                    if current_len < min_length:
                        min_length = current_len
                        merge_index = i
                
                # åˆå¹¶æ‰¾åˆ°çš„æœ€çŸ­ç›¸é‚»åˆ†æ®µ
                merged_segment = result[merge_index] + " " + result[merge_index + 1]
                result = result[:merge_index] + [merged_segment] + result[merge_index + 2:]

            # æœ€ç»ˆæ£€æŸ¥æ™ºèƒ½åˆ†å‰²ç»“æœæ˜¯å¦åˆç†
            if (len(result) > 1 and 
                all(count_words(segment) >= 3 for segment in result)):
                
                # æ£€æŸ¥åˆ†æ®µæ˜¯å¦å¹³è¡¡ï¼ˆå·®è·ä¸è¦å¤ªå¤§ï¼‰
                lengths = [count_words(segment) for segment in result]
                if max(lengths) <= min(lengths) * 3:  # é•¿åº¦æ¯”ä¾‹åˆç†
                    logger.debug(f"æ™ºèƒ½åˆ†å‰²: {' -- '.join(result)}")
                    return result

    # æ™ºèƒ½åˆ†å‰²å¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨å¼ºåˆ¶åˆ†å‰²ç­–ç•¥
    logger.warning(f"âš ï¸ æ™ºèƒ½åˆ†å‰²å¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨å¼ºåˆ¶åˆ†å‰²ç­–ç•¥: {text[:50]}...")
    
    # å¤‡ç”¨ç­–ç•¥1: å°è¯•åœ¨ä¸­é—´ä½ç½®å¯»æ‰¾è¾ƒå¥½çš„åˆ†å‰²ç‚¹
    mid_point = len(words) // 2
    best_split = mid_point
    
    # åœ¨ä¸­é—´ä½ç½®å‰å5ä¸ªè¯çš„èŒƒå›´å†…å¯»æ‰¾è¾ƒå¥½çš„åˆ†å‰²ç‚¹
    search_range = 5
    start_search = max(3, mid_point - search_range)
    end_search = min(len(words) - 3, mid_point + search_range)
    
    # ä¼˜å…ˆé€‰æ‹©æ ‡ç‚¹ç¬¦å·åçš„ä½ç½®
    for i in range(start_search, end_search + 1):
        if i < len(words) and words[i-1].rstrip().endswith(('.', ',', ';', ':', '!', '?')):
            best_split = i
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼Œé€‰æ‹©è¿æ¥è¯å‰çš„ä½ç½®
    if best_split == mid_point:
        for i in range(start_search, end_search + 1):
            if i < len(words):
                word = words[i].lower().strip(",.!?")
                if word in {"and", "or", "but", "so", "then", "however", "therefore"}:
                    best_split = i
                    break
    
    # æ‰§è¡Œå¤‡ç”¨åˆ†å‰²
    first_part = " ".join(words[:best_split])
    second_part = " ".join(words[best_split:])
    
    result = [first_part, second_part]
    logger.warning(f"ğŸ”§ å¼ºåˆ¶åˆ†å‰²å®Œæˆ: {' -- '.join(result)}")
    return result