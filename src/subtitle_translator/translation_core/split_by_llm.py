import re
from typing import List
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI

from .data import SubtitleSegment
from .prompts import SPLIT_SYSTEM_PROMPT
from .config import SubtitleConfig, get_default_config
from ..logger import setup_logger

logger = setup_logger("split_by_llm")

def count_words(text: str) -> int:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­è‹±æ–‡å•è¯æ•°
    Args:
        text: è¾“å…¥æ–‡æœ¬ï¼Œè‹±æ–‡
    Returns:
        int: è‹±æ–‡å•è¯æ•°
    """
    english_text = re.sub(r'[\u4e00-\u9fff]', ' ', text)
    english_words = english_text.strip().split()
    return len(english_words)

def split_by_end_marks(sentence: str) -> List[str]:
    """
    æŒ‰å¥å­ç»“æŸæ ‡è®°ï¼ˆå¥å·ã€æ„Ÿå¹å·ç­‰ï¼‰æ‹†åˆ†å¥å­ï¼Œåªè¦æœ‰ç»“æŸæ ‡è®°å°±åˆ†å‰²
    
    Args:
        sentence: éœ€è¦æ‹†åˆ†çš„å¥å­
        
    Returns:
        List[str]: æ‹†åˆ†åçš„å¥å­åˆ—è¡¨
    """
    # å®šä¹‰ç»“æŸæ ‡è®°ï¼Œæ³¨æ„æ¯ä¸ªæ ‡è®°åé¢éƒ½åŠ äº†ç©ºæ ¼
    end_marks = [". ", "! ", "? ", "... ", "â€¦â€¦ ", "; ", "? ", "! "]
    positions = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æŸæ ‡è®°çš„ä½ç½®
    for mark in end_marks:
        start = 0
        while True:
            pos = sentence.find(mark, start)
            if pos == -1:
                break
            # ç¡®ä¿ä¸æ˜¯å°æ•°ç‚¹
            if mark == ". " and pos > 0:
                prev_char = sentence[pos-1]
                if prev_char.isdigit():
                    start = pos + 1
                    continue
            positions.append((pos + len(mark.strip()), mark))
            start = pos + 1
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œè¿”å›åŸå¥å­
    if not positions:
        return [sentence]
    
    # æŒ‰ä½ç½®æ’åºå¹¶æ‰§è¡Œåˆ†å‰²
    positions.sort()
    segments = []
    start = 0
    
    for pos, mark in positions:
        segment = sentence[start:pos].strip()
        if segment:  # åªè¦ä¸æ˜¯ç©ºå­—ç¬¦ä¸²å°±æ·»åŠ 
            segments.append(segment)
        start = pos
    
    # æ·»åŠ æœ€åä¸€æ®µ
    last_segment = sentence[start:].strip()
    if last_segment:
        segments.append(last_segment)
    
    # è®°å½•æ—¥å¿—
    if len(segments) > 1:
        logger.info(f"æ‹†åˆ†ä¼˜åŒ–: {' -- '.join(segments)}")
    
    return segments if segments else [sentence]

def split_by_llm(text: str,
                model: str = None,
                max_word_count_english: int = 14,
                max_retries: int = 3,
                batch_index: int = None) -> List[str]:
    """
    ä½¿ç”¨LLMæ‹†åˆ†å¥å­
    
    Args:
        text: è¦æ‹†åˆ†çš„æ–‡æœ¬
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
        max_word_count_english: è‹±æ–‡æœ€å¤§å•è¯æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        
    Returns:
        List[str]: æ‹†åˆ†åçš„å¥å­åˆ—è¡¨
    """
    logger.info(f"ğŸ“ å¤„ç†æ–‡æœ¬: å…±{count_words(text)}ä¸ªå•è¯")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    config = SubtitleConfig()
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ–­å¥æ¨¡å‹
    if model is None:
        model = config.split_model
    
    client = OpenAI(
        base_url=config.openai_base_url,
        api_key=config.openai_api_key
    )
    
    # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯
    system_prompt = SPLIT_SYSTEM_PROMPT.replace("[max_word_count_english]", str(max_word_count_english))
    
    # åœ¨ç”¨æˆ·æç¤ºä¸­æ·»åŠ å¯¹ç©ºæ ¼çš„å¼ºè°ƒ
    user_prompt = f"Please use multiple <br> tags to separate the following sentence. Make sure to preserve all spaces and punctuation exactly as they appear in the original text:\n{text}"

    try:
        # è°ƒç”¨API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.2,
            timeout=80
        )
        
        # å¤„ç†å“åº”
        result = response.choices[0].message.content
        if not result:
            raise Exception("APIè¿”å›ä¸ºç©º")
        logger.debug(f"APIè¿”å›ç»“æœ: \n\n{result}\n")

        # æ¸…ç†å’Œåˆ†å‰²æ–‡æœ¬ - ç®€åŒ–å¤„ç†ï¼Œä¿ç•™åŸå§‹æ ¼å¼
        result = re.sub(r'\n+', '', result)
        
        # ç›´æ¥æŒ‰<br>åˆ†å‰²ï¼Œä¿ç•™åŸå§‹æ ¼å¼å’Œç©ºæ ¼
        sentences = result.split("<br>")
        
        # æ¸…ç†ç©ºç™½è¡Œï¼Œä½†ä¿ç•™å†…éƒ¨ç©ºæ ¼
        sentences = [seg.strip() for seg in sentences if seg.strip()]

        # éªŒè¯å¥å­é•¿åº¦
        new_sentences = []
        long_sentence_count = 0
        super_long_count = 0
        
        for sentence in sentences:
            # é¦–å…ˆæŒ‰ç»“æŸæ ‡è®°æ‹†åˆ†å¥å­
            segments = split_by_end_marks(sentence)
            
            # å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œé•¿åº¦æ£€æŸ¥
            for segment in segments:
                threshold = max_word_count_english + 5
                word_count = count_words(segment)
                
                if max_word_count_english < word_count < threshold:
                    long_sentence_count += 1
                    logger.debug(f"âš ï¸ é•¿å¥: {word_count}å­— - {segment[:30]}...")
                elif word_count > threshold:
                    super_long_count += 1
                    logger.info(f"ğŸ”„ è¶…é•¿å¥åˆ†å‰²: {word_count}å­— - {segment[:30]}...")
                    # å°è¯•åˆ‡åˆ†å¥å­
                    split_results = split_by_common_words(segment)
                    new_sentences.extend(split_results)
                else:
                    new_sentences.append(segment)
        
        sentences = new_sentences

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if long_sentence_count > 0:
            logger.info(f"ğŸ“Š å‘ç° {long_sentence_count} ä¸ªé•¿å¥")
        if super_long_count > 0:
            logger.info(f"âœ‚ï¸ è‡ªåŠ¨åˆ†å‰² {super_long_count} ä¸ªè¶…é•¿å¥")

        # éªŒè¯ç»“æœ
        word_count = count_words(text)
        expected_segments = word_count / max_word_count_english
        actual_segments = len(sentences)
        
        if actual_segments < expected_segments * 0.9:
            logger.warning(f"âš ï¸ æ–­å¥æ•°é‡ä¸è¶³ï¼šé¢„æœŸ {expected_segments:.1f}ï¼Œå®é™… {actual_segments}")
        
        batch_prefix = f"[æ‰¹æ¬¡{batch_index}]" if batch_index else ""
        logger.info(f"âœ… {batch_prefix} æ–­å¥å®Œæˆ: {len(sentences)} ä¸ªå¥å­")
        return sentences
        
    except Exception as e:
        if max_retries > 0:
            logger.warning(f"APIè°ƒç”¨å¤±è´¥: {str(e)}ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {max_retries-1}")
            return split_by_llm(text, model, max_word_count_english, max_retries-1, batch_index)
        else:
            logger.error(f"APIè°ƒç”¨å¤±è´¥, æ— æ³•æ‹†åˆ†å¥å­: {str(e)}")
            # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å¥å­æ‹†åˆ†
            return text.split(". ")
        
def split_by_common_words(text: str) -> List[str]:
    """
    åœ¨å¸¸è§è¿æ¥è¯å¤„å¯¹å¥å­è¿›è¡Œåˆ†å‰²

    Args:
        text: éœ€è¦åˆ†å‰²çš„å¥å­
    Returns:
        åˆ†å‰²åçš„å¥å­åˆ—è¡¨ï¼Œå¦‚æœæ— æ³•åˆ†å‰²åˆ™è¿”å›åŒ…å«åŸå¥å­çš„åˆ—è¡¨
    """
    # å®šä¹‰åœ¨è¯è¯­å‰é¢åˆ†å‰²çš„å¸¸è§è¯
    prefix_split_words = {
        # è¿æ¥è¯å’Œä»‹è¯
        "and", "or", "but", "if", "then", "because", "as", "until",
        "while", "what", "when", "where", "nor", "yet", "so", "for",
        "however", "moreover", "furthermore", "additionally", "besides",
        "therefore", "thus", "hence", "consequently",
        # ä»å¥å¼•å¯¼è¯
        "that", "which", "who", "whom", "whose", "why", "how",
        # æ—¶é—´å’Œæ¡ä»¶
        "before", "after", "since", "while", "unless", "although",
        "though", "even though", "whereas", "whether",
        # ç›®çš„å’Œç»“æœ
        "in order to", "so that", "such that", "in case", "provided that",
        # å¯¹æ¯”å’Œè®©æ­¥
        "despite", "in spite of", "rather than", "instead of",
        # è¡¥å……å’Œé€’è¿›
        "also", "besides", "in addition", "moreover", "furthermore",
        "similarly", "likewise", "meanwhile", "subsequently"
    }

    # å®šä¹‰åœ¨è¯è¯­åé¢åˆ†å‰²çš„å¸¸è§è¯
    suffix_split_words = {
        # æ ‡ç‚¹ç¬¦å·
        ".", ",", "!", "?", ":", ";", "...", "â€¦",
        # å¼•å·å’Œæ‹¬å·
        "\"", "'", "'", "'", "'", "'", ")", "]", "}"
    }

    # é¢„å¤„ç†æ–‡æœ¬
    text = text.strip()
    words = text.split()
    
    # å¦‚æœå¥å­å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
    if len(words) < 8:
        return [text]
        
    # å¯»æ‰¾åˆ†å‰²ç‚¹
    split_positions = []
    for i, word in enumerate(words):
        word_lower = word.lower().strip(",.!?")
        # æ£€æŸ¥å‰ç¼€åˆ†å‰²è¯
        if i > 2 and i < len(words) - 2:  # ç¡®ä¿ä¸åœ¨å¥å­å¼€å¤´å’Œç»“å°¾å¤ªè¿‘çš„ä½ç½®
            # æ£€æŸ¥å•è¯å’ŒçŸ­è¯­
            for prefix in prefix_split_words:
                if " " in prefix:  # å¤„ç†å¤šè¯çŸ­è¯­
                    if i + len(prefix.split()) <= len(words):
                        phrase = " ".join(words[i:i+len(prefix.split())]).lower()
                        if phrase == prefix:
                            split_positions.append(i)
                            break
                elif word_lower == prefix:  # å¤„ç†å•ä¸ªè¯
                    split_positions.append(i)
                    break
        
        # æ£€æŸ¥åç¼€åˆ†å‰²è¯
        if i > 2 and i < len(words) - 1:  # ç¡®ä¿ä¸åœ¨å¥å­å¼€å¤´å¤ªè¿‘çš„ä½ç½®
            if word_lower in suffix_split_words:
                split_positions.append(i + 1)  # åœ¨åç¼€è¯ä¹‹ååˆ†å‰²
    
    # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œè¿”å›åŸå¥å­
    if not split_positions:
        return [text]
    
    # æ’åºå¹¶å»é‡åˆ†å‰²ç‚¹
    split_positions = sorted(list(set(split_positions)))
    
    # æ‰§è¡Œåˆ†å‰²
    result = []
    start = 0
    for pos in split_positions:
        if pos - start >= 3:  # ç¡®ä¿æ¯ä¸ªåˆ†æ®µè‡³å°‘æœ‰3ä¸ªè¯
            segment = " ".join(words[start:pos])
            if segment:
                result.append(segment)
            start = pos
    
    # æ·»åŠ æœ€åä¸€ä¸ªåˆ†æ®µ
    if start < len(words):
        last_segment = " ".join(words[start:])
        if last_segment:
            result.append(last_segment)
    
    # å¦‚æœåˆ†å‰²ç»“æœä¸ç†æƒ³ï¼ˆæ²¡æœ‰åˆ†æ®µæˆ–åªæœ‰ä¸€ä¸ªåˆ†æ®µï¼‰ï¼Œè¿”å›åŸå¥å­
    if len(result) <= 1:
        return [text]

    # å¦‚æœæœ‰å¤šäºä¸¤ä¸ªåˆ†æ®µï¼Œå°è¯•åˆå¹¶æœ€çŸ­çš„ç›¸é‚»åˆ†æ®µ
    while len(result) > 2:
        # æ‰¾å‡ºæœ€çŸ­çš„ç›¸é‚»åˆ†æ®µå¯¹
        min_length = float('inf')
        merge_index = 0
        
        for i in range(len(result) - 1):
            current_len = count_words(result[i]) + count_words(result[i + 1])
            if current_len < min_length:
                min_length = current_len
                merge_index = i
        
        # åˆå¹¶æ‰¾åˆ°çš„æœ€çŸ­ç›¸é‚»åˆ†æ®µ
        merged_segment = result[merge_index] + " " + result[merge_index + 1]
        result = result[:merge_index] + [merged_segment] + result[merge_index + 2:]

    # æœ€ç»ˆæ£€æŸ¥ä¸¤ä¸ªåˆ†æ®µæ˜¯å¦åˆç†
    if any(count_words(segment) < 3 for segment in result):
        return [text]
    
    # æ£€æŸ¥åˆ†æ®µæ˜¯å¦å¹³è¡¡ï¼ˆå·®è·ä¸è¦å¤ªå¤§ï¼‰
    lengths = [count_words(segment) for segment in result]
    if max(lengths) > min(lengths) * 3:  # å¦‚æœæœ€é•¿çš„åˆ†æ®µè¶…è¿‡æœ€çŸ­çš„3å€
        return [text]
    logger.info(f"åˆ†å‰²ä¼˜åŒ–: {' -- '.join(result)}")
    return result