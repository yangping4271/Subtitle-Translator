import os
from dataclasses import dataclass, field
from typing import Optional

from .terminology import DEFAULT_TERMINOLOGY

# è¯­è¨€ä»£ç æ˜ å°„è¡¨
LANGUAGE_MAPPING = {
    "zh": "ç®€ä½“ä¸­æ–‡",
    "zh-cn": "ç®€ä½“ä¸­æ–‡", 
    "zh-tw": "ç¹ä½“ä¸­æ–‡",
    "ja": "æ—¥æ–‡",
    "japanese": "æ—¥æ–‡",
    "en": "English",
    "english": "English",
    "ko": "éŸ©æ–‡",
    "korean": "éŸ©æ–‡",
    "fr": "æ³•æ–‡",
    "french": "æ³•æ–‡",
    "de": "å¾·æ–‡",
    "german": "å¾·æ–‡",
    "es": "è¥¿ç­ç‰™æ–‡",
    "spanish": "è¥¿ç­ç‰™æ–‡",
    "pt": "è‘¡è„ç‰™æ–‡",
    "portuguese": "è‘¡è„ç‰™æ–‡",
    "ru": "ä¿„æ–‡",
    "russian": "ä¿„æ–‡",
    "it": "æ„å¤§åˆ©æ–‡",
    "italian": "æ„å¤§åˆ©æ–‡",
    "ar": "é˜¿æ‹‰ä¼¯æ–‡",
    "arabic": "é˜¿æ‹‰ä¼¯æ–‡",
    "th": "æ³°æ–‡",
    "thai": "æ³°æ–‡",
    "vi": "è¶Šå—æ–‡",
    "vietnamese": "è¶Šå—æ–‡",
}

def get_target_language(lang_code: str) -> str:
    """å°†è¯­è¨€ä»£ç è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€åç§°"""
    if not lang_code or not isinstance(lang_code, str):
        raise ValueError(f"è¯­è¨€ä»£ç ä¸èƒ½ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹: '{lang_code}'")

    lang_code = lang_code.lower().strip()
    if lang_code in LANGUAGE_MAPPING:
        return LANGUAGE_MAPPING[lang_code]

    language_groups = {
        "ä¸­æ–‡": ["zh", "zh-cn", "zh-tw"],
        "äºšæ´²è¯­è¨€": ["ja", "ko", "th", "vi"],
        "æ¬§æ´²è¯­è¨€": ["en", "fr", "de", "es", "pt", "it", "ru"],
        "å…¶ä»–è¯­è¨€": ["ar"]
    }

    error_msg = f"âŒ ä¸æ”¯æŒçš„è¯­è¨€ä»£ç : '{lang_code}'\n\nğŸŒ æ”¯æŒçš„è¯­è¨€ä»£ç :\n"
    for group_name, codes in language_groups.items():
        group_codes = [code for code in codes if code in LANGUAGE_MAPPING]
        if group_codes:
            error_msg += f"\nğŸ“‚ {group_name}:\n"
            for code in group_codes:
                lang_name = LANGUAGE_MAPPING[code]
                error_msg += f"   {code:6} -> {lang_name}\n"

    suggestions = {
        "jp": ["ja"], "kr": ["ko"], "cn": ["zh", "zh-cn"],
        "chinese": ["zh", "zh-cn"], "japanese": ["ja"], "korean": ["ko"],
        "english": ["en"], "french": ["fr"], "german": ["de"],
        "spanish": ["es"], "portuguese": ["pt"], "russian": ["ru"],
        "italian": ["it"], "arabic": ["ar"], "thai": ["th"],
        "vietnamese": ["vi"],
    }

    similar_codes = []
    if lang_code in suggestions:
        similar_codes = suggestions[lang_code]
    else:
        for supported_code in LANGUAGE_MAPPING.keys():
            if (lang_code in supported_code or supported_code in lang_code or
                abs(len(lang_code) - len(supported_code)) <= 1):
                similar_codes.append(supported_code)

    if similar_codes:
        error_msg += f"\nğŸ’¡ æ‚¨æ˜¯å¦æƒ³è¦ä½¿ç”¨: {', '.join(similar_codes[:3])}"

    error_msg += f"\n\nğŸ“Š æ€»è®¡æ”¯æŒ {len(set(LANGUAGE_MAPPING.values()))} ç§è¯­è¨€ï¼Œ{len(LANGUAGE_MAPPING)} ä¸ªè¯­è¨€ä»£ç "
    raise ValueError(error_msg)

@dataclass
class SubtitleConfig:
    """å­—å¹•å¤„ç†é…ç½®ç±»"""
    openai_base_url: str = ""
    openai_api_key: str = ""
    llm_model: str = "gpt-4o-mini"

    split_model: str = "gpt-4o-mini"
    summary_model: str = "gpt-4o-mini"
    translation_model: str = "gpt-4o"

    target_language: str = "ç®€ä½“ä¸­æ–‡"
    max_word_count_english: int = 19
    thread_num: int = 18

    min_batch_sentences: int = 15
    max_batch_sentences: int = 25
    target_batch_sentences: int = 20

    tolerance_multiplier: float = 1.2
    warning_multiplier: float = 1.5
    max_multiplier: float = 2.0

    need_reflect: bool = False

    terminology: Optional[dict] = None

    _skip_env_load: bool = field(default=False, repr=False)

    def set_target_language(self, lang_code: str) -> None:
        """è®¾ç½®ç›®æ ‡è¯­è¨€"""
        self.target_language = get_target_language(lang_code)

    def __post_init__(self):
        """éªŒè¯é…ç½®å¹¶é‡æ–°è¯»å–ç¯å¢ƒå˜é‡"""
        if self._skip_env_load:
            return

        self.openai_base_url = os.getenv('OPENAI_BASE_URL', '')
        self.openai_api_key = os.getenv('OPENAI_API_KEY', '')
        self.llm_model = os.getenv('LLM_MODEL', self.llm_model)

        self.split_model = os.getenv('SPLIT_MODEL', self.llm_model)
        self.summary_model = os.getenv('SUMMARY_MODEL', self.llm_model)
        self.translation_model = os.getenv('TRANSLATION_MODEL', self.llm_model)

        env_target_lang = os.getenv('TARGET_LANGUAGE')
        if env_target_lang:
            try:
                self.set_target_language(env_target_lang)
            except ValueError:
                pass

        if self.terminology is None:
            self.terminology = DEFAULT_TERMINOLOGY.get(self.target_language, {})

        if not self.openai_base_url or not self.openai_api_key:
            error_msg = f"ç¯å¢ƒå˜é‡éªŒè¯å¤±è´¥:\n"
            error_msg += f"  OPENAI_BASE_URL = '{self.openai_base_url}' (é•¿åº¦: {len(self.openai_base_url)})\n"
            error_msg += f"  OPENAI_API_KEY = '{self.openai_api_key[:20]}...' (é•¿åº¦: {len(self.openai_api_key)})\n"
            error_msg += f"  LLM_MODEL = '{self.llm_model}'\n"
            error_msg += f"  SPLIT_MODEL = '{self.split_model}'\n"
            error_msg += f"  SUMMARY_MODEL = '{self.summary_model}'\n"
            error_msg += f"  TRANSLATION_MODEL = '{self.translation_model}'"
            raise ValueError(error_msg)

# æ–‡ä»¶ç›¸å…³å¸¸é‡
SRT_SUFFIX = ".srt"

# å»¶è¿Ÿåˆ›å»ºé»˜è®¤é…ç½®å®ä¾‹
def get_default_config() -> SubtitleConfig:
    """è·å–é»˜è®¤é…ç½®å®ä¾‹"""
    return SubtitleConfig() 