#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäº spaCy çš„æ™ºèƒ½å¥å­åˆ†å‰²æ¨¡å—
ä½¿ç”¨ä¾å­˜å…³ç³»åˆ†æè¿›è¡Œè¯­æ³•æ„ŸçŸ¥çš„å¥å­åˆ†å‰²
"""

import re
from typing import List, Tuple, Optional
from ..logger import setup_logger

logger = setup_logger("spacy_splitter")

# å…¨å±€å˜é‡ï¼Œç”¨äºæ‡’åŠ è½½ spaCy æ¨¡å‹
_nlp = None
_spacy_available = None

def _is_spacy_available() -> bool:
    """æ£€æŸ¥ spaCy æ˜¯å¦å¯ç”¨"""
    global _spacy_available
    if _spacy_available is not None:
        return _spacy_available
    
    try:
        import spacy
        _spacy_available = True
        return True
    except ImportError:
        logger.debug("spaCy æœªå®‰è£…ï¼Œå°†è·³è¿‡è¯­æ³•åˆ†æåˆ†å‰²")
        _spacy_available = False
        return False

def _load_spacy_model():
    """æ‡’åŠ è½½ spaCy æ¨¡å‹"""
    global _nlp
    if _nlp is not None:
        return _nlp
    
    if not _is_spacy_available():
        return None
    
    try:
        import spacy
        _nlp = spacy.load("en_core_web_sm")
        logger.debug("spaCy è‹±æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ")
        return _nlp
    except OSError:
        logger.debug("æœªæ‰¾åˆ° en_core_web_sm æ¨¡å‹ï¼Œè·³è¿‡ spaCy åˆ†å‰²")
        return None
    except Exception as e:
        logger.debug(f"spaCy æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def count_words(text: str) -> int:
    """ç»Ÿè®¡è‹±æ–‡å•è¯æ•°"""
    english_text = re.sub(r'[\u4e00-\u9fff]', ' ', text)
    english_words = english_text.strip().split()
    return len(english_words)

def find_split_points(doc) -> List[Tuple[int, str, str]]:
    """
    æŸ¥æ‰¾å¥å­ä¸­çš„æœ€ä½³åˆ†å‰²ç‚¹
    
    Args:
        doc: spaCy å¤„ç†åçš„æ–‡æ¡£å¯¹è±¡
        
    Returns:
        åˆ†å‰²ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (tokenç´¢å¼•, è¯è¯­, åˆ†å‰²åŸå› )
    """
    split_points = []
    tokens = list(doc)
    
    for i, token in enumerate(tokens):
        # è·³è¿‡å¥é¦–å¥å°¾é™„è¿‘çš„token
        if i < 2 or i >= len(tokens) - 2:
            continue
        
        # 1. ä»å±è¿è¯ï¼ˆå¼•å¯¼çŠ¶è¯­ä»å¥ï¼‰- ä¼˜å…ˆçº§æœ€é«˜
        if (token.dep_ == "mark" and 
            token.text.lower() in ["because", "since", "when", "where", "if", "unless", 
                                 "although", "though", "while", "whereas", "whether"]):
            split_points.append((i, token.text, "ä»å±è¿è¯"))
        
        # 2. å¹¶åˆ—è¿è¯ + å®Œæ•´å¥å­ç»“æ„ - éœ€è¦éªŒè¯å‰åéƒ½æœ‰åŠ¨è¯
        elif (token.dep_ == "cc" and 
              token.text.lower() in ["and", "but", "or", "so", "yet"]):
            
            # æ£€æŸ¥å‰åæ˜¯å¦æœ‰åŠ¨è¯ç»“æ„
            has_verb_before = any(t.pos_ == "VERB" for t in tokens[:i] if i - t.i <= 10)
            has_verb_after = any(t.pos_ == "VERB" for t in tokens[i+1:] if t.i - i <= 10)
            
            if has_verb_before and has_verb_after:
                split_points.append((i, token.text, "å¹¶åˆ—è¿è¯+åŠ¨è¯ç»“æ„"))
        
        # 3. å¹¶åˆ—åŠ¨è¯ï¼ˆconjå…³ç³»çš„åŠ¨è¯ï¼‰
        elif token.dep_ == "conj" and token.pos_ == "VERB":
            split_points.append((i, token.text, "å¹¶åˆ—åŠ¨è¯"))
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šä»å±è¿è¯ > å¹¶åˆ—è¿è¯+åŠ¨è¯ç»“æ„ > å¹¶åˆ—åŠ¨è¯
    priority_order = {"ä»å±è¿è¯": 1, "å¹¶åˆ—è¿è¯+åŠ¨è¯ç»“æ„": 2, "å¹¶åˆ—åŠ¨è¯": 3}
    split_points.sort(key=lambda x: (priority_order.get(x[2], 4), x[0]))
    
    return split_points

def validate_split(segments: List[str], min_words: int = 3) -> bool:
    """
    éªŒè¯åˆ†å‰²ç»“æœæ˜¯å¦åˆç†
    
    Args:
        segments: åˆ†å‰²åçš„å¥å­ç‰‡æ®µ
        min_words: æ¯ä¸ªç‰‡æ®µçš„æœ€å°è¯æ•°
        
    Returns:
        True å¦‚æœåˆ†å‰²åˆç†
    """
    if len(segments) < 2:
        return False
    
    # æ£€æŸ¥æ¯ä¸ªç‰‡æ®µçš„é•¿åº¦
    for segment in segments:
        if count_words(segment.strip()) < min_words:
            return False
    
    # æ£€æŸ¥é•¿åº¦å¹³è¡¡æ€§ï¼ˆæœ€é•¿ä¸è¶…è¿‡æœ€çŸ­çš„4å€ï¼‰
    lengths = [count_words(seg.strip()) for seg in segments]
    if max(lengths) > min(lengths) * 4:
        return False
    
    return True

def spacy_split(text: str, max_segments: int = 3) -> List[str]:
    """
    ä½¿ç”¨ spaCy è¿›è¡ŒåŸºäºè¯­æ³•åˆ†æçš„æ™ºèƒ½å¥å­åˆ†å‰²
    
    Args:
        text: éœ€è¦åˆ†å‰²çš„å¥å­
        max_segments: æœ€å¤§åˆ†å‰²ç‰‡æ®µæ•°
        
    Returns:
        åˆ†å‰²åçš„å¥å­åˆ—è¡¨ï¼Œå¦‚æœåˆ†å‰²å¤±è´¥è¿”å›ç©ºåˆ—è¡¨
    """
    # åŠ è½½æ¨¡å‹
    nlp = _load_spacy_model()
    if nlp is None:
        return []
    
    try:
        # åˆ†æå¥å­ç»“æ„
        doc = nlp(text)
        tokens = [token.text for token in doc]
        
        # æŸ¥æ‰¾åˆ†å‰²ç‚¹
        split_points = find_split_points(doc)
        
        if not split_points:
            logger.debug("æœªæ‰¾åˆ°åˆé€‚çš„è¯­æ³•åˆ†å‰²ç‚¹")
            return []
        
        # é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹
        selected_points = []
        
        # ä¼˜å…ˆé€‰æ‹©ä»å±è¿è¯
        subordinate_points = [p for p in split_points if p[2] == "ä»å±è¿è¯"]
        if subordinate_points:
            selected_points.append(subordinate_points[0])
        
        # å¦‚æœéœ€è¦æ›´å¤šåˆ†å‰²ç‚¹ï¼Œæ·»åŠ å¹¶åˆ—è¿è¯
        if len(selected_points) < max_segments - 1:
            conj_points = [p for p in split_points if p[2] == "å¹¶åˆ—è¿è¯+åŠ¨è¯ç»“æ„"]
            if conj_points:
                # é€‰æ‹©è·ç¦»å·²é€‰ç‚¹æœ€è¿œçš„å¹¶åˆ—è¿è¯
                if selected_points:
                    existing_pos = selected_points[0][0]
                    best_conj = max(conj_points, key=lambda p: abs(p[0] - existing_pos))
                else:
                    best_conj = conj_points[0]
                selected_points.append(best_conj)
        
        # å¦‚æœä»éœ€è¦åˆ†å‰²ç‚¹ï¼Œæ·»åŠ å¹¶åˆ—åŠ¨è¯
        if len(selected_points) < max_segments - 1:
            verb_points = [p for p in split_points if p[2] == "å¹¶åˆ—åŠ¨è¯"]
            if verb_points and len(selected_points) < 2:
                selected_points.append(verb_points[0])
        
        if not selected_points:
            return []
        
        # æŒ‰ä½ç½®æ’åº
        selected_points.sort(key=lambda x: x[0])
        
        # æ‰§è¡Œåˆ†å‰²
        segments = []
        start = 0
        
        for point_idx, word, reason in selected_points:
            segment = " ".join(tokens[start:point_idx]).strip()
            if segment:
                segments.append(segment)
            start = point_idx
        
        # æ·»åŠ æœ€åä¸€æ®µ
        last_segment = " ".join(tokens[start:]).strip()
        if last_segment:
            segments.append(last_segment)
        
        # éªŒè¯åˆ†å‰²ç»“æœ
        if validate_split(segments):
            reason_info = ", ".join([f"{word}({reason})" for _, word, reason in selected_points])
            logger.info(f"ğŸ§  spaCyåˆ†å‰²æˆåŠŸ: {reason_info} -> {len(segments)}æ®µ")
            return segments
        else:
            logger.debug("spaCyåˆ†å‰²ç»“æœéªŒè¯å¤±è´¥")
            return []
            
    except Exception as e:
        logger.debug(f"spaCyåˆ†å‰²å¼‚å¸¸: {e}")
        return []

def get_spacy_info() -> str:
    """è·å– spaCy çŠ¶æ€ä¿¡æ¯"""
    if not _is_spacy_available():
        return "spaCyæœªå®‰è£…"
    
    nlp = _load_spacy_model()
    if nlp is None:
        return "spaCyå¯ç”¨ï¼Œä½†en_core_web_smæ¨¡å‹æœªæ‰¾åˆ°"
    
    return "spaCyå°±ç»ª"